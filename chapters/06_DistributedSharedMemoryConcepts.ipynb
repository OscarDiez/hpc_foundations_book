{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7.1: Introduction to OpenMP\n",
        "\n",
        "OpenMP (Open Multi-Processing) is a powerful API designed for parallel programming in shared-memory environments. This exercise will introduce you to the basics of OpenMP, including how to use OpenMP directives to parallelize loops and how to manage thread parallelism.\n",
        "\n",
        "## 7.1.1 Overview of OpenMP\n",
        "\n",
        "### 7.1.1.1 Definition and Purpose of OpenMP\n",
        "OpenMP provides a simple and flexible interface for developing parallel applications by using compiler directives, runtime library routines, and environment variables. It allows developers to parallelize existing serial code incrementally, making it easier to transition from sequential to parallel programming.\n",
        "\n",
        "### 7.1.1.2 Historical Context and Development\n",
        "OpenMP was first introduced in 1997 and has since evolved with support for task-based parallelism, accelerator directives, and memory management improvements, making it a relevant tool in modern HPC environments.\n",
        "\n",
        "### 7.1.1.3 Applicability in Modern HPC Environments\n",
        "OpenMP is widely applicable in modern HPC due to its ability to leverage multicore architectures efficiently. It is used in scientific simulations, data analysis, and real-time processing.\n",
        "\n",
        "## 7.1.2 Key Features of OpenMP\n",
        "\n",
        "### 7.1.2.1 Simple and Flexible Parallel Programming Model\n",
        "OpenMP simplifies parallel programming by allowing developers to parallelize loops with minimal code changes. For example, using the `#pragma omp parallel for` directive to parallelize a loop.\n",
        "\n",
        "### 7.1.2.2 Support for C, C++, and Fortran\n",
        "OpenMP supports multiple programming languages, including C, C++, and Fortran, which broadens its applicability across various scientific and engineering domains.\n",
        "\n",
        "### 7.1.2.3 Portable Across Different Shared-Memory Architectures\n",
        "OpenMP is portable across various shared-memory architectures, ensuring that parallel code can run efficiently on different systems without modification.\n",
        "\n",
        "## 7.1.3 Installation and Setup of OpenMP\n",
        "\n",
        "### 7.1.3.1 Installing OpenMP on Various Platforms\n",
        "- **Linux:** Use GCC with the `-fopenmp` flag to compile OpenMP programs.\n",
        "- **Windows:** Use MinGW or Visual Studio to enable OpenMP support.\n",
        "- **MacOS:** Use Homebrew to install GCC for OpenMP support.\n",
        "\n",
        "### 7.1.3.2 Compiler Support for OpenMP\n",
        "OpenMP is supported by GCC, Clang, and Intel Compilers, each providing robust support for parallel programming.\n",
        "\n",
        "## 7.1.3.3 Thread Parallelism\n",
        "Thread parallelism in OpenMP divides a task into smaller sub-tasks that can be executed by multiple threads simultaneously. This approach leverages multicore processors for efficient parallel execution.\n"
      ],
      "metadata": {
        "id": "ijCS7T0wd6BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this program, we use `#pragma omp parallel` to instruct the compiler to parallelize the enclosed code block. The `omp_get_thread_num()` function returns the thread number executing the current block, which helps us verify that multiple threads are indeed running the code concurrently.\n"
      ],
      "metadata": {
        "id": "8Spyo2sEwTgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a simple C code for OpenMP Hello World\n",
        "hello_world_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    // Parallel region with OpenMP\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        // Each thread prints \"Hello World\"\n",
        "        printf(\"Hello World from thread %d\\\\n\", omp_get_thread_num());\n",
        "    }\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Saving the code to a file\n",
        "with open(\"hello_world.c\", \"w\") as f:\n",
        "    f.write(hello_world_code)\n",
        "\n",
        "# Compile the program with OpenMP support\n",
        "!gcc -fopenmp hello_world.c -o hello_world\n",
        "\n",
        "# Run the program\n",
        "!./hello_world\n"
      ],
      "metadata": {
        "id": "keIxS8RswT9Z",
        "outputId": "4d8c6477-6fff-42a5-b356-c709af8e7487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from thread 1\n",
            "Hello World from thread 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a simple C code for OpenMP Hello World with thread count input\n",
        "hello_world_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int num_threads;\n",
        "\n",
        "    // Prompting the user to input the number of threads\n",
        "    printf(\"Enter the number of threads: \");\n",
        "    scanf(\"%d\", &num_threads);\n",
        "\n",
        "    // Set the number of threads for OpenMP\n",
        "    omp_set_num_threads(num_threads);\n",
        "\n",
        "    // Parallel region with OpenMP\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        // Each thread prints \"Hello World\"\n",
        "        printf(\"Hello World from thread %d out of %d\\\\n\", omp_get_thread_num(), omp_get_num_threads());\n",
        "    }\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Saving the code to a file\n",
        "with open(\"hello_world.c\", \"w\") as f:\n",
        "    f.write(hello_world_code)\n",
        "\n",
        "# Compile the program with OpenMP support\n",
        "!gcc -fopenmp hello_world.c -o hello_world\n",
        "\n",
        "# Run the program (students will input the number of threads at runtime)\n",
        "!./hello_world\n"
      ],
      "metadata": {
        "id": "1MJJGFEBxic9",
        "outputId": "8b0a2bd5-f2ce-460e-954d-dc5f11e77535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of threads: 4\n",
            "Hello World from thread 1 out of 4\n",
            "Hello World from thread 2 out of 4\n",
            "Hello World from thread 3 out of 4\n",
            "Hello World from thread 0 out of 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "40-cRmgg7Qah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we parallelize a loop that computes the sum of the first N numbers. We use the `#pragma omp parallel for` directive to split the loop iterations across different threads. The `reduction(+:sum)` clause ensures that the partial sums from each thread are safely combined into a final result.\n"
      ],
      "metadata": {
        "id": "7GyFRPJHw4lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a simple OpenMP program to parallelize a loop\n",
        "loop_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int N = 1000;\n",
        "    int sum = 0;\n",
        "\n",
        "    // Parallelize this loop\n",
        "    #pragma omp parallel for reduction(+:sum)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += i;\n",
        "    }\n",
        "\n",
        "    printf(\"Sum of first %d numbers is %d\\\\n\", N, sum);\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the code to a file\n",
        "with open(\"parallel_loop.c\", \"w\") as f:\n",
        "    f.write(loop_code)\n",
        "\n",
        "# Compile the program with OpenMP support\n",
        "!gcc -fopenmp parallel_loop.c -o parallel_loop\n",
        "\n",
        "# Run the program\n",
        "!./parallel_loop\n"
      ],
      "metadata": {
        "id": "bR7JZeodw1ED",
        "outputId": "4feb46b3-dc46-4345-8276-9f36cb9ff694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of first 1000 numbers is 499500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serial vs Parallel Code Using OpenMP\n",
        "\n",
        "In this notebook, we will compare two versions of a simple program that adds elements of two arrays. The first version is a serial implementation, where the entire task is done sequentially by a single thread. The second version is a parallel implementation using OpenMP, where the array operations are divided among multiple threads.\n",
        "\n",
        "Understanding the difference between serial and parallel execution is crucial in HPC, as it demonstrates how parallelization can improve performance by utilizing multiple CPU cores simultaneously.\n",
        "\n",
        "### **Serial Code Explanation**\n",
        "\n",
        "In the serial code, we create two arrays `a[]` and `b[]`, initialize them, and then add their corresponding elements to form a result array `result[]`. The entire process is done sequentially, and only one thread (the main thread) performs the operations.\n",
        "\n",
        "### **Parallel Code Explanation**\n",
        "\n",
        "In the parallel code, we use OpenMP to divide the array addition task among multiple threads. Each thread performs the addition for a different portion of the arrays. This is done using the `#pragma omp parallel` directive, which forks multiple threads, and `#pragma omp for`, which divides the loop iterations among those threads.\n",
        "\n",
        "Parallelization helps reduce execution time when dealing with larger datasets by utilizing multiple CPU cores effectively. The program also prints which thread is working on which index, allowing us to see how work is distributed across threads.\n",
        "\n",
        "### **Key Points in Parallel Code**:\n",
        "- `#pragma omp parallel`: This directive is used to start parallel execution. Threads are created here.\n",
        "- `#pragma omp for`: This distributes the loop iterations among the available threads.\n",
        "- `omp_get_thread_num()`: This function returns the thread ID, allowing us to print which thread is working on a particular iteration.\n",
        "\n",
        "In the OpenMP code, the work of adding the arrays is done in parallel, making it faster for larger data sets. However, the correctness of the result remains the same.\n"
      ],
      "metadata": {
        "id": "QkkQTmVjyHpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Serial code\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "    const int N = 20;\n",
        "    int i;\n",
        "    double a[N], b[N], result[N];\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (i = 0; i < N; i++) {\n",
        "        a[i] = 1.0 * i;\n",
        "        b[i] = 2.0 * i;\n",
        "    }\n",
        "\n",
        "    // Perform element-wise addition\n",
        "    for (i = 0; i < N; i++) {\n",
        "        result[i] = a[i] + b[i];\n",
        "    }\n",
        "\n",
        "    // Print test result\n",
        "    printf(\"TEST result[19] = %g\\\\n\", result[19]);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the serial code to a file\n",
        "with open(\"serial_code.c\", \"w\") as f:\n",
        "    f.write(serial_code)\n",
        "\n",
        "# Compile the serial program\n",
        "!gcc serial_code.c -o serial_code\n",
        "\n",
        "# Run the serial program\n",
        "!./serial_code\n"
      ],
      "metadata": {
        "id": "xk081RCXyLte",
        "outputId": "75f836f6-7cb1-4e65-9d7a-4240d8582804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST result[19] = 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenMP parallel code\n",
        "openmp_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "    const int N = 20;\n",
        "    int i;\n",
        "    double a[N], b[N], result[N];\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (i = 0; i < N; i++) {\n",
        "        a[i] = 1.0 * i;\n",
        "        b[i] = 2.0 * i;\n",
        "    }\n",
        "\n",
        "    // Parallel region begins here\n",
        "    #pragma omp parallel private(i)\n",
        "    {\n",
        "        int threadid = omp_get_thread_num();  // Get thread id\n",
        "\n",
        "        // Parallel loop: each thread processes part of the arrays\n",
        "        #pragma omp for\n",
        "        for (i = 0; i < N; i++) {\n",
        "            result[i] = a[i] + b[i];\n",
        "            printf(\"Thread id: %d working on index %d\\\\n\", threadid, i);\n",
        "        }\n",
        "    } // Parallel region ends\n",
        "\n",
        "    // Print test result\n",
        "    printf(\"TEST result[19] = %g\\\\n\", result[19]);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the OpenMP code to a file\n",
        "with open(\"openmp_code.c\", \"w\") as f:\n",
        "    f.write(openmp_code)\n",
        "\n",
        "# Compile the parallel program with OpenMP support\n",
        "!gcc -fopenmp openmp_code.c -o openmp_code\n",
        "\n",
        "# Run the OpenMP program\n",
        "!./openmp_code\n"
      ],
      "metadata": {
        "id": "pMPZztx0yLZU",
        "outputId": "e7878173-e35c-485d-a0cd-0b2b95139b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread id: 0 working on index 0\n",
            "Thread id: 0 working on index 1\n",
            "Thread id: 0 working on index 2\n",
            "Thread id: 0 working on index 3\n",
            "Thread id: 0 working on index 4\n",
            "Thread id: 0 working on index 5\n",
            "Thread id: 0 working on index 6\n",
            "Thread id: 0 working on index 7\n",
            "Thread id: 0 working on index 8\n",
            "Thread id: 0 working on index 9\n",
            "Thread id: 1 working on index 10\n",
            "Thread id: 1 working on index 11\n",
            "Thread id: 1 working on index 12\n",
            "Thread id: 1 working on index 13\n",
            "Thread id: 1 working on index 14\n",
            "Thread id: 1 working on index 15\n",
            "Thread id: 1 working on index 16\n",
            "Thread id: 1 working on index 17\n",
            "Thread id: 1 working on index 18\n",
            "Thread id: 1 working on index 19\n",
            "TEST result[19] = 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same codes but measuring the time."
      ],
      "metadata": {
        "id": "p_3W93Tg0SHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Serial code with time measurement using clock()\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>  // For measuring time using clock()\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "    const int N = 300000;  // Reduce the size for testing purposes\n",
        "    int i;\n",
        "    double a[N], b[N], result[N];\n",
        "    clock_t start_time, end_time;\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (i = 0; i < N; i++) {\n",
        "        a[i] = 1.0 * i;\n",
        "        b[i] = 2.0 * i;\n",
        "    }\n",
        "\n",
        "    // Start measuring time\n",
        "    start_time = clock();\n",
        "\n",
        "    // Perform element-wise addition\n",
        "    for (i = 0; i < N; i++) {\n",
        "        result[i] = a[i] + b[i];\n",
        "    }\n",
        "\n",
        "    // End measuring time\n",
        "    end_time = clock();\n",
        "\n",
        "    // Print test result and time taken\n",
        "    printf(\"TEST result[N-1] = %g\\\\n\", result[N-1]);\n",
        "    fflush(stdout);  // Ensure immediate output\n",
        "    printf(\"Time taken by serial code: %f seconds\\\\n\", (double)(end_time - start_time) / CLOCKS_PER_SEC);\n",
        "    fflush(stdout);  // Ensure immediate output\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the serial code to a file\n",
        "with open(\"serial_code.c\", \"w\") as f:\n",
        "    f.write(serial_code)\n",
        "\n",
        "# Compile the serial program\n",
        "!gcc serial_code.c -o serial_code\n",
        "\n",
        "# Run the serial program\n",
        "!./serial_code\n"
      ],
      "metadata": {
        "id": "_1SJNrJxybe1",
        "outputId": "4819a4a2-9578-4704-a2f4-8c412ff16afc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST result[N-1] = 899997\n",
            "Time taken by serial code: 0.001035 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenMP parallel code with time measurement\n",
        "openmp_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "    const int N = 300000;  // Reduce the size for testing purposes\n",
        "    int i;\n",
        "    double a[N], b[N], result[N];\n",
        "    double start_time, end_time;\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (i = 0; i < N; i++) {\n",
        "        a[i] = 1.0 * i;\n",
        "        b[i] = 2.0 * i;\n",
        "    }\n",
        "\n",
        "    // Start measuring time\n",
        "    start_time = omp_get_wtime();\n",
        "\n",
        "    // Parallel region begins here\n",
        "    #pragma omp parallel private(i)\n",
        "    {\n",
        "        int threadid = omp_get_thread_num();  // Get thread id\n",
        "\n",
        "        // Parallel loop: each thread processes part of the arrays\n",
        "        #pragma omp for\n",
        "        for (i = 0; i < N; i++) {\n",
        "            result[i] = a[i] + b[i];\n",
        "        }\n",
        "    } // Parallel region ends\n",
        "\n",
        "    // End measuring time\n",
        "    end_time = omp_get_wtime();\n",
        "\n",
        "    // Print test result and time taken\n",
        "    printf(\"TEST result[N-1] = %g\\\\n\", result[N-1]);\n",
        "    fflush(stdout);  // Ensure immediate output\n",
        "    printf(\"Time taken by OpenMP code: %f seconds\\\\n\", end_time - start_time);\n",
        "    fflush(stdout);  // Ensure immediate output\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the OpenMP code to a file\n",
        "with open(\"openmp_code.c\", \"w\") as f:\n",
        "    f.write(openmp_code)\n",
        "\n",
        "# Compile the parallel program with OpenMP support\n",
        "!gcc -fopenmp openmp_code.c -o openmp_code\n",
        "\n",
        "# Run the OpenMP program\n",
        "!./openmp_code\n"
      ],
      "metadata": {
        "id": "hXHJpVBpybTF",
        "outputId": "a7339c8a-3179-457e-d46b-6e48a81bba7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST result[N-1] = 899997\n",
            "Time taken by OpenMP code: 0.000827 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Modifying OpenMP Code with Time Measurement\n",
        "\n",
        "In this exercise, you will modify the existing OpenMP code to better understand how time measurement and thread management work in OpenMP.\n",
        "\n",
        "### Task: Measure Time for Each Thread\n",
        "The current program measures the total execution time of the parallel region, but it doesn't give any information about the time each thread takes to complete its work. Modify the program so that:\n",
        "1. Each thread measures its own execution time within the parallel region.\n",
        "2. Print the execution time for each thread after the parallel loop.\n",
        "\n",
        "### Hint:\n",
        "- Use `omp_get_wtime()` within the parallel region to measure the time at the start and end of the thread's execution.\n",
        "- You can print the thread ID and its execution time inside the parallel block after the loop.\n",
        "\n",
        "After making these changes, run the program and observe how the execution time varies between threads.\n"
      ],
      "metadata": {
        "id": "B9qxj0ya72rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7.2: OpenMP Directives and Clauses\n",
        "\n",
        "This exercise will guide you through the use of OpenMP directives and clauses, focusing on parallel regions, controlling the number of threads, and data sharing among threads.\n",
        "\n",
        "## 7.2.1 Parallel Regions\n",
        "A parallel region in OpenMP is a block of code that runs simultaneously across multiple threads. This is initiated using the `#pragma omp parallel` directive.\n",
        "\n",
        "### 7.2.1.1 num_threads Clause\n",
        "The `num_threads` clause specifies the exact number of threads to be used in the parallel region. This is important for optimizing performance and ensuring proper resource utilization.\n",
        "\n",
        "### 7.2.1.2 default Clause\n",
        "The `default` clause specifies the default data-sharing attributes for variables within a parallel region. It can be set to `shared`, `private`, or `none`, determining how variables are accessed by threads.\n",
        "\n",
        "## 7.2.2 Assigning the Number of Threads\n",
        "Assigning the number of threads can be done inside the code using the `num_threads` clause or outside the code using environment variables. Both methods have their own use cases and advantages.\n",
        "\n",
        "## 7.2.3 Work-sharing Constructs\n",
        "Work-sharing constructs in OpenMP, like `#pragma omp for` and `#pragma omp sections`, are used to divide tasks among threads, allowing for efficient parallel execution.\n"
      ],
      "metadata": {
        "id": "RDSK_fHYeKKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using OpenMP directives and clauses\n",
        "# Save this C code to a file named \"omp_directives.c\"\n",
        "\n",
        "omp_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    #pragma omp parallel num_threads(4)\n",
        "    {\n",
        "        int id = omp_get_thread_num();\n",
        "        int num_threads = omp_get_num_threads();\n",
        "        if (id == 0) {\n",
        "            printf(\"Total number of threads: %d\\\\n\", num_threads);\n",
        "        }\n",
        "        printf(\"Thread %d is running\\\\n\", id);\n",
        "    }\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write the OpenMP code to a file\n",
        "with open('omp_directives.c', 'w') as f:\n",
        "    f.write(omp_code)\n",
        "\n",
        "# Compile the OpenMP C code using GCC with the -fopenmp flag\n",
        "!gcc -fopenmp -o omp_directives omp_directives.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./omp_directives\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtpdreA9eMor",
        "outputId": "ffb5c2f7-3658-4b00-83f6-ed0e412a3087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread 3 is running\n",
            "Thread 2 is running\n",
            "Total number of threads: 4\n",
            "Thread 0 is running\n",
            "Thread 1 is running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenMP Program with Parallel Sections\n",
        "\n",
        "In this example, we will explore how OpenMP can be used to parallelize different tasks using sections. This code performs the following tasks:\n",
        "1. Initializes an array `x[]` with values from 0 to 99.\n",
        "2. Divides the array into two groups based on a `divide` threshold.\n",
        "3. Calculates the sum of the elements in the array.\n",
        "4. Calculates the sum of the squares of the elements in the array.\n",
        "\n",
        "Each of these tasks is done in parallel using OpenMP sections. This is an example of **task parallelism** where different threads work on different parts of the computation simultaneously.\n",
        "\n",
        "### **Key OpenMP Concepts Used**\n",
        "- **`#pragma omp parallel for`**: This directive is used to parallelize the initialization of the array `x[]`. Each thread will initialize a portion of the array.\n",
        "- **`#pragma omp sections`**: This is used to split the program into different sections, where each section is executed by a separate thread.\n",
        "- **Shared and Private Variables**: The variable `x[]` is shared between threads, while `i` is private for each thread, meaning that each thread has its own copy of `i`.\n",
        "\n",
        "### **Code Explanation**\n",
        "- The array `x[]` is first initialized using a parallel loop.\n",
        "- The program then forks into different threads using `#pragma omp sections`. Each thread works on a different section:\n",
        "    - One thread counts the number of values in `x[]` that are below or above a given threshold (`divide`).\n",
        "    - Another thread calculates the sum of all the elements in `x[]`.\n",
        "    - A third thread calculates the sum of the squares of the elements in `x[]`.\n",
        "\n",
        "By parallelizing these tasks, we can speed up the program and utilize multiple cores effectively.\n",
        "\n",
        "### **C Code with OpenMP Sections**\n"
      ],
      "metadata": {
        "id": "IQnCYh5K4k3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the C code to a file\n",
        "openmp_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "    const int N = 100;\n",
        "    int x[N], i, sum, sum2;\n",
        "    int upper = 0, lower = 0;  // Initialize upper and lower\n",
        "    int divide = 20;\n",
        "    sum = 0;\n",
        "    sum2 = 0;\n",
        "\n",
        "    // Parallelize the initialization of the array\n",
        "    #pragma omp parallel for\n",
        "    for (i = 0; i < N; i++) {\n",
        "        x[i] = i;\n",
        "    }\n",
        "\n",
        "    // Parallel region with sections\n",
        "    #pragma omp parallel private(i) shared(x, upper, lower)\n",
        "    {\n",
        "        // Fork several different threads using sections\n",
        "        #pragma omp sections\n",
        "        {\n",
        "            // First section: Count elements below and above the divide threshold\n",
        "            #pragma omp section\n",
        "            {\n",
        "                for (i = 0; i < N; i++) {\n",
        "                    if (x[i] > divide) upper++;\n",
        "                    if (x[i] <= divide) lower++;\n",
        "                }\n",
        "                printf(\"The number of points at or below %d in x is %d\\\\n\", divide, lower);\n",
        "                printf(\"The number of points above %d in x is %d\\\\n\", divide, upper);\n",
        "            }\n",
        "\n",
        "            // Second section: Calculate the sum of elements in x\n",
        "            #pragma omp section\n",
        "            {\n",
        "                for (i = 0; i < N; i++) {\n",
        "                    sum = sum + x[i];\n",
        "                }\n",
        "                printf(\"Sum of x = %d\\\\n\", sum);\n",
        "            }\n",
        "\n",
        "            // Third section: Calculate the sum of squares of elements in x\n",
        "            #pragma omp section\n",
        "            {\n",
        "                for (i = 0; i < N; i++) {\n",
        "                    sum2 = sum2 + x[i] * x[i];\n",
        "                }\n",
        "                printf(\"Sum2 of x = %d\\\\n\", sum2);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Save the C code to a file\n",
        "with open(\"openmp_code.c\", \"w\") as file:\n",
        "    file.write(openmp_code)\n",
        "\n",
        "# Step 3: Compile the C code with OpenMP support\n",
        "!gcc -fopenmp openmp_code.c -o openmp_code\n",
        "\n",
        "# Step 4: Run the compiled program\n",
        "!./openmp_code\n"
      ],
      "metadata": {
        "id": "NLbVBvdl4mX3",
        "outputId": "53cfa354-6301-4731-9c7e-1a916118c23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of points at or below 20 in x is 21\n",
            "The number of points above 20 in x is 79\n",
            "Sum2 of x = 328350\n",
            "Sum of x = 4950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Modifying OpenMP Sections with Shared and Private Variables\n",
        "\n",
        "In this exercise, you will modify the existing OpenMP code to better understand how shared and private variables work in OpenMP, especially when using sections.\n",
        "\n",
        "### Task: Fix the Issue with Shared Variables\n",
        "Currently, the variables `upper` and `lower` are shared among all threads. However, multiple threads are trying to modify these shared variables simultaneously, which can cause incorrect results (race conditions). Modify the program so that:\n",
        "1. Each thread has its own private copy of `upper` and `lower`.\n",
        "2. After each thread finishes its work, the results should be combined into the shared `upper` and `lower` variables in a safe manner.\n",
        "\n",
        "### Hint:\n",
        "- Use the `private` clause to make `upper` and `lower` private within the sections.\n",
        "- Use the `reduction` clause or a critical section to safely combine the results from each thread.\n",
        "\n",
        "After making these changes, run the program and check whether the results for `upper` and `lower` are correct.\n"
      ],
      "metadata": {
        "id": "eo7AYZqT8PUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7.3: Data Environment in OpenMP\n",
        "\n",
        "This exercise explores the data-sharing clauses in OpenMP, including `shared`, `private`, `firstprivate`, and `reduction`, which control how variables are accessed and modified within parallel regions.\n",
        "\n",
        "## 7.3.1 Data Sharing Clauses\n",
        "### 7.3.1.1 Shared Clause\n",
        "The `shared` clause makes a variable accessible to all threads in a parallel region, which can lead to race conditions if not synchronized properly.\n",
        "\n",
        "### 7.3.1.2 Private Clause\n",
        "The `private` clause ensures that each thread has its own instance of a variable, which is useful for thread-specific computations.\n",
        "\n",
        "### 7.3.1.3 Firstprivate Clause\n",
        "The `firstprivate` clause initializes private variables with the value from the master thread, ensuring consistent initial states across threads.\n",
        "\n",
        "## 7.3.2 Reduction Clause\n",
        "The `reduction` clause is used to perform a reduction operation (e.g., sum, product) on variables across all threads, combining their results into a single value.\n"
      ],
      "metadata": {
        "id": "QBiu65VXeO8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using data-sharing clauses in OpenMP\n",
        "# Save this C code to a file named \"omp_data_clauses.c\"\n",
        "\n",
        "omp_data_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int n = 1000;\n",
        "    int sum = 0;\n",
        "    int array[1000];\n",
        "\n",
        "    // Initialize the array\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        array[i] = i + 1;\n",
        "    }\n",
        "\n",
        "    #pragma omp parallel for reduction(+:sum)\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        sum += array[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Total Sum: %d\\\\n\", sum); // Correct total sum\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write the OpenMP code to a file\n",
        "with open('omp_data_clauses.c', 'w') as f:\n",
        "    f.write(omp_data_code)\n",
        "\n",
        "# Compile the OpenMP C code using GCC with the -fopenmp flag\n",
        "!gcc -fopenmp -o omp_data_clauses omp_data_clauses.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./omp_data_clauses\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09EWXueGeRCj",
        "outputId": "28acab8c-bd14-464a-ba65-676871ad38b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sum: 500500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Understanding Data-Sharing Clauses in OpenMP\n",
        "\n",
        "In this exercise, you will modify the existing OpenMP code to explore the behavior of data-sharing clauses in OpenMP, particularly focusing on shared and private variables.\n",
        "\n",
        "### Task: Modify the Code to Make the Array Private\n",
        "The current program calculates the sum of an array using OpenMP with the `reduction` clause. However, the array `array[]` is shared across all threads by default. Modify the program so that:\n",
        "1. Each thread has its own private copy of the array during the parallel loop.\n",
        "2. After the parallel loop, ensure that the final result is still correct.\n",
        "\n",
        "### Hint:\n",
        "- You can use the `private` or `firstprivate` clause to make the array private to each thread.\n",
        "- Ensure that the initialization of the array happens outside the parallel region, or make sure the array is initialized properly in each thread if you use `firstprivate`.\n",
        "\n",
        "Run the program after making these changes and compare the results. This will help you understand how data-sharing clauses work in OpenMP.\n"
      ],
      "metadata": {
        "id": "J4kTQBd88U3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7.4: Synchronization Techniques in OpenMP\n",
        "\n",
        "In this exercise, you will learn about synchronization techniques in OpenMP, including critical sections, atomic operations, barriers, and locks.\n",
        "\n",
        "## 7.4.1 Critical Sections\n",
        "A critical section is a block of code that must be executed by only one thread at a time, ensuring mutual exclusion.\n",
        "\n",
        "## 7.4.2 Atomic Operations\n",
        "Atomic operations provide a lightweight synchronization mechanism for simple updates to shared variables.\n",
        "\n",
        "## 7.4.3 Barrier Synchronization\n",
        "The `#pragma omp barrier` directive ensures that all threads reach a specific point before any can proceed, useful for coordinating tasks.\n",
        "\n",
        "## 7.4.4 Locks\n",
        "Locks provide fine-grained control over access to critical sections, allowing threads to acquire and release locks manually.\n"
      ],
      "metadata": {
        "id": "SLhCRrUSebTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of synchronization techniques in OpenMP\n",
        "# Save this C code to a file named \"omp_sync.c\"\n",
        "\n",
        "omp_sync_code = \"\"\"\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int balance = 0;\n",
        "    omp_lock_t lock;\n",
        "\n",
        "    omp_init_lock(&lock);\n",
        "\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < 1000; i++) {\n",
        "        omp_set_lock(&lock);\n",
        "        balance += 1;\n",
        "        omp_unset_lock(&lock);\n",
        "    }\n",
        "\n",
        "    omp_destroy_lock(&lock);\n",
        "\n",
        "    printf(\"Final Balance: %d\\\\n\", balance);\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write the OpenMP code to a file\n",
        "with open('omp_sync.c', 'w') as f:\n",
        "    f.write(omp_sync_code)\n",
        "\n",
        "# Compile the OpenMP C code using GCC with the -fopenmp flag\n",
        "!gcc -fopenmp -o omp_sync omp_sync.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./omp_sync\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f728xresejPZ",
        "outputId": "85be59c1-c75c-447f-b762-cb7729e890dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Balance: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Understanding Synchronization Techniques in OpenMP\n",
        "\n",
        "In this exercise, you will modify an existing OpenMP program to explore how synchronization works in OpenMP. The program currently uses locks (`omp_set_lock()` and `omp_unset_lock()`) to ensure that multiple threads do not modify the `balance` variable simultaneously.\n",
        "\n",
        "### Task: Replace Locks with Critical Sections\n",
        "In the current program, locks are used to ensure that the `balance` is updated correctly. Modify the program so that:\n",
        "1. You remove the lock mechanism.\n",
        "2. Instead of using locks, use the `#pragma omp critical` directive to ensure that only one thread at a time modifies the `balance`.\n",
        "\n",
        "### Hint:\n",
        "- Use the `#pragma omp critical` directive in place of the lock around the `balance` update.\n",
        "- Test the program and check if the final balance remains correct.\n",
        "\n",
        "### Task 2: Experiment with Race Conditions (Optional)\n",
        "Remove the `critical` directive entirely and observe the effect of the race condition when multiple threads try to modify the `balance` without any synchronization. Discuss why the result is incorrect.\n",
        "\n",
        "Run the modified program and observe the results. This will help you understand the importance of synchronization techniques like locks and critical sections in OpenMP.\n"
      ],
      "metadata": {
        "id": "5RKXx6bl88cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Parallelizing Code with OpenMP\n",
        "\n",
        "In this exercise, you will learn how to parallelize a code step by step using OpenMP. The goal is to transform a serial program into a parallel program by applying OpenMP directives.\n",
        "\n",
        "You are given a serial code that performs the following tasks:\n",
        "1. Initializes an array with values.\n",
        "2. Computes the sum of all elements in the array.\n",
        "3. Computes the sum of squares of all elements in the array.\n",
        "\n",
        "### **Steps to Parallelize the Code**\n",
        "\n",
        "#### **Step 1: Analyze the Serial Code**\n",
        "First, we provide you with the serial version of the code. Review it carefully. Your task is to parallelize the initialization, summation, and sum of squares calculation using OpenMP.\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000000;  // Size of the array\n",
        "    int i;\n",
        "    double x[N], sum = 0.0, sum2 = 0.0;\n",
        "\n",
        "    // Step 1: Initialize the array\n",
        "    for (i = 0; i < N; i++) {\n",
        "        x[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Step 2: Calculate the sum of all elements\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum += x[i];\n",
        "    }\n",
        "\n",
        "    // Step 3: Calculate the sum of the squares of all elements\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum2 += x[i] * x[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Sum: %f\\n\", sum);\n",
        "    printf(\"Sum of squares: %f\\n\", sum2);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "###Step 2: Add OpenMP Directives\n",
        "Now, try to parallelize the following parts of the code using OpenMP:\n",
        "\n",
        "Initialization of the array: Use the #pragma omp parallel for directive to parallelize the loop that initializes the array.\n",
        "Summing the elements: Add the #pragma omp parallel for directive to parallelize the summation loop.\n",
        "Summing the squares: Again, use #pragma omp parallel for to parallelize the loop that computes the sum of squares.\n",
        "Keep in mind:\n",
        "\n",
        "You need to ensure thread safety when multiple threads are updating shared variables (sum, sum2). You can achieve this by using the OpenMP reduction clause.\n",
        "\n",
        "Step 3: Check for Correctness\n",
        "Once you've parallelized the code, run it and check whether the output matches the serial version. Make sure that the final sum and sum of squares are correct.\n",
        "\n",
        "After you've attempted the exercise, we'll provide the solution."
      ],
      "metadata": {
        "id": "0jPGgKsw5u3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Writing the parallelized C code with OpenMP to a file\n",
        "parallel_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "\n",
        "# Add your code here\n",
        "\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the parallel code to a file\n",
        "with open(\"parallel_code.c\", \"w\") as file:\n",
        "    file.write(parallel_code)\n",
        "\n",
        "# Compile the parallel code with OpenMP support\n",
        "!gcc -fopenmp parallel_code.c -o parallel_code\n",
        "\n",
        "# Run the compiled parallel program\n",
        "!./parallel_code\n"
      ],
      "metadata": {
        "id": "xP12G6uu5w9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "###Solution to the exercise"
      ],
      "metadata": {
        "id": "d_BnbO6e6FRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serial Code so you can run it:"
      ],
      "metadata": {
        "id": "UBYRv_K06zwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Writing the serial version of the C code to a file (for students to parallelize)\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000000;  // Size of the array\n",
        "    int i;\n",
        "    double x[N], sum = 0.0, sum2 = 0.0;\n",
        "\n",
        "    // Step 1: Initialize the array\n",
        "    for (i = 0; i < N; i++) {\n",
        "        x[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Step 2: Calculate the sum of all elements\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum += x[i];\n",
        "    }\n",
        "\n",
        "    // Step 3: Calculate the sum of the squares of all elements\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum2 += x[i] * x[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Sum: %f\\\\n\", sum);\n",
        "    printf(\"Sum of squares: %f\\\\n\", sum2);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the serial code to a file\n",
        "with open(\"serial_code.c\", \"w\") as file:\n",
        "    file.write(serial_code)\n",
        "\n",
        "# Compile the serial code (no OpenMP needed here)\n",
        "!gcc serial_code.c -o serial_code\n",
        "\n",
        "# Run the serial program\n",
        "!./serial_code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASGqMS3Z6B-i",
        "outputId": "840be16a-5c2f-4314-9d2e-28758656c21c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 499999500000.000000\n",
            "Sum of squares: 333332833333127552.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and the solutions with the parallel code"
      ],
      "metadata": {
        "id": "hkMcNf-Q62nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Writing the parallelized C code with OpenMP to a file\n",
        "parallel_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000000;  // Size of the array\n",
        "    int i;\n",
        "    double x[N], sum = 0.0, sum2 = 0.0;\n",
        "\n",
        "    // Step 1: Parallelize array initialization\n",
        "    #pragma omp parallel for\n",
        "    for (i = 0; i < N; i++) {\n",
        "        x[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Step 2: Parallelize the summation with reduction\n",
        "    #pragma omp parallel for reduction(+:sum)\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum += x[i];\n",
        "    }\n",
        "\n",
        "    // Step 3: Parallelize the sum of squares with reduction\n",
        "    #pragma omp parallel for reduction(+:sum2)\n",
        "    for (i = 0; i < N; i++) {\n",
        "        sum2 += x[i] * x[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Sum: %f\\\\n\", sum);\n",
        "    printf(\"Sum of squares: %f\\\\n\", sum2);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the parallel code to a file\n",
        "with open(\"parallel_code.c\", \"w\") as file:\n",
        "    file.write(parallel_code)\n",
        "\n",
        "# Compile the parallel code with OpenMP support\n",
        "!gcc -fopenmp parallel_code.c -o parallel_code\n",
        "\n",
        "# Run the compiled parallel program\n",
        "!./parallel_code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KacwsHD96k66",
        "outputId": "d073f93f-6d98-450c-af6a-4706ff8d0df4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 499999500000.000000\n",
            "Sum of squares: 333332833333020928.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Explanation of the Solution\n",
        "Parallelizing the Initialization: The #pragma omp parallel for directive is used to distribute the loop iterations among threads. Each thread will initialize a portion of the array.\n",
        "\n",
        "Parallelizing the Sum Calculation: Since multiple threads will be updating the sum variable, we use the reduction(+:sum) clause. This ensures that each thread will maintain a local copy of sum, and at the end of the parallel region, these local sums will be combined.\n",
        "\n",
        "Parallelizing the Sum of Squares: Similar to the sum calculation, the reduction(+:sum2) clause is used to handle the concurrent updates to the sum2 variable."
      ],
      "metadata": {
        "id": "6lWni8_j6Mhg"
      }
    }
  ]
}