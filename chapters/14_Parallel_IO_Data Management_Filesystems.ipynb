{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Filesystems in HPC\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In high-performance computing (HPC), distributed filesystems play a crucial role in providing scalable and efficient storage solutions across multiple nodes. One common approach is using Network File Systems (NFS) to allow file sharing between different machines in a cluster. Distributed filesystems ensure that users can access and share data seamlessly across the cluster nodes, which is essential for parallel processing and data-intensive workloads.\n",
        "\n",
        "In this tutorial, we will explore how NFS works in an HPC environment, how to mount a distributed filesystem, and how users can interact with such a system. You will learn:\n",
        "\n",
        "1. **What is NFS and its role in HPC clusters**.\n",
        "2. **How to set up and mount an NFS share**.\n",
        "3. **How to copy and manage files within a distributed filesystem**.\n",
        "4. **Best practices for using distributed filesystems in HPC**.\n",
        "\n",
        "This tutorial will be executed in Google Colab, simulating a distributed filesystem. For demonstration, we will install required software, create a simple NFS setup, and interact with the filesystem using basic commands.\n",
        "\n",
        "---\n",
        "\n",
        "## What is NFS?\n",
        "\n",
        "NFS, or Network File System, allows a computer to share directories and files with others over a network. It enables users on remote machines to interact with files on a server as if they were local, making it a popular choice in HPC for sharing large datasets across nodes.\n",
        "\n",
        "The main steps include:\n",
        "\n",
        "- Configuring the server to export a directory.\n",
        "- Configuring clients to mount that directory over the network.\n",
        "- Copying, accessing, and managing files as if they were local.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m7nXxIwyXh8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking NFS Mounts in an HPC Cluster\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In an HPC (High-Performance Computing) environment, it's common to use a Network File System (NFS) to share directories across multiple nodes in the cluster. NFS allows users to access files and directories stored on a remote server as if they were on the local machine. Understanding how NFS works and how to check for mounted NFS directories is crucial for managing and using shared resources efficiently in an HPC environment.\n",
        "\n",
        "In this section, we will walk through the process of connecting to the cluster via SSH and checking the NFS mounts available on your system.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Connecting to the Cluster via SSH\n",
        "\n",
        "To interact with the HPC cluster and check for NFS mounts, you first need to connect to one of the cluster's nodes. You can do this using the `ssh` (Secure Shell) command, which allows you to securely log into a remote system.\n",
        "\n",
        "### Example Command:\n",
        "```bash\n",
        "ssh username@cluster_address\n",
        "```\n",
        "Here, replace username with your cluster login username and cluster_address with the IP address or domain name of the cluster login node."
      ],
      "metadata": {
        "id": "UGwFWHVPXiyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Checking for NFS Mounts\n",
        "Once logged into the cluster, you can use the mount command to check for mounted NFS directories. This will show you all the currently mounted filesystems, including any NFS shares.\n",
        "\n",
        "Example Command:\n",
        "\n",
        "```bash\n",
        "mount | grep nfs\n",
        "```\n",
        "This command filters the output of mount to show only NFS mounts. If NFS is being used in your cluster, you will see entries like the following:\n",
        "\n",
        "Example Output:\n",
        "```bash\n",
        "10.0.1.6:/home on /home type nfs4 (rw,nosuid,noatime,seclabel,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,acregmax=3,acdirmin=3,acdirmax=3,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.1.4,local_lock=none,addr=10.0.1.6,_netdev,x-systemd.automount)\n",
        "10.0.1.6:/project on /project type nfs4 (rw,nosuid,noatime,seclabel,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,acregmax=3,acdirmin=3,acdirmax=3,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.1.4,local_lock=none,addr=10.0.1.6,_netdev,x-systemd.automount)\n",
        "10.0.1.6:/scratch on /scratch type nfs4 (rw,nosuid,noatime,seclabel,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,acregmax=3,acdirmin=3,acdirmax=3,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.1.4,local_lock=none,addr=10.0.1.6,_netdev,x-systemd.automount)\n",
        "```\n",
        "\n",
        "###Step 3: Understanding the Output\n",
        "NFS Server: The IP address or hostname of the NFS server is the first part of each line, such as 10.0.1.6:/home.\n",
        "\n",
        "Mount Point: The location on the local node where the NFS share is mounted, for example /home, /project, or /scratch.\n",
        "\n",
        "NFS Version: The version of NFS being used is indicated, such as nfs4.\n",
        "\n",
        "Mount Options: These are the options used for the NFS mount, which control read/\n",
        "write permissions, timeouts, and other settings. For example, rw (read/write), noatime (no access time update), vers=4.2 (NFS version 4.2), proto=tcp (using TCP), and many others.\n",
        "\n",
        "Example Commands:\n",
        "List all mounted filesystems:\n",
        "\n",
        "```bash\n",
        "mount\n",
        "```\n",
        "This command shows all mounted filesystems, including NFS, local disks, and other mounts.\n",
        "\n",
        "View only NFS mounts:\n",
        "\n",
        "```bash\n",
        "mount | grep nfs\n",
        "```\n",
        "Filters the output to show only NFS mounts.\n",
        "\n",
        "Check disk usage of NFS mounts:\n",
        "\n",
        "```bash\n",
        "df -hT | grep nfs\n",
        "```\n",
        "Displays disk space usage for mounted NFS directories, including the size, used space, and free space.\n",
        "\n"
      ],
      "metadata": {
        "id": "0qjXrLmVaPJq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YAKCBlldsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel I/O with MPI-IO and HDF5\n",
        "\n",
        "In high-performance computing (HPC) environments, efficient I/O operations are critical to overall performance. **Parallel I/O** refers to the ability of multiple processes to read and write data simultaneously to a shared file. This is particularly important for scientific applications that work with large datasets.\n",
        "\n",
        "In this example, we will demonstrate two parallel I/O methods using:\n",
        "1. **MPI-IO**: A low-level interface provided by the MPI library that allows parallel read/write operations to a shared file.\n",
        "2. **HDF5**: A high-level data format designed for large-scale data management, which supports parallel I/O.\n",
        "\n",
        "Both methods will be executed on an HPC cluster using multiple processes, and data will be written in parallel to a shared file.\n",
        "\n",
        "### Objectives:\n",
        "- Learn how to use MPI-IO to write data in parallel from multiple processes.\n",
        "- Understand the benefits of using HDF5 for parallel data management.\n",
        "\n",
        "### Requirements:\n",
        "- The cluster needs to have **MPI**, **HDF5**, and **parallel file systems** like Lustre installed.\n",
        "- This example assumes a multi-node setup with JupyterLab access on the HPC cluster.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_xSfoKUHT0-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel I/O with MPI-IO and HDF5 (Cluster one)\n",
        "### Running Parallel HDF5 Example on the Cluster\n",
        "\n",
        "This guide will walk you through connecting to the cluster, setting up the environment, compiling an HDF5 example program, and running it.\n",
        "\n",
        "### Step 1: Connect to the Cluster via SSH\n",
        "\n",
        "To connect to the cluster, open a terminal and use the `ssh` command. Replace `username` and `cluster_address` with the appropriate values provided by your system administrator.\n",
        "\n",
        "```bash\n",
        "ssh username@cluster_address\n"
      ],
      "metadata": {
        "id": "Fhc1SOlSWi9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once connected, you will be in your home directory on the cluster.\n",
        "\n",
        "###Step 2: Load the Necessary Modules\n",
        "Before compiling and running your HDF5 example, you need to load the required modules for the environment. Run the following commands:\n",
        "\n",
        "```bash\n",
        "module load gcc/9.3.0\n",
        "module load openmpi/4.0.3\n",
        "module load hdf5-mpi/1.12.1\n",
        "```\n",
        "You can check the currently loaded modules using:\n",
        "\n",
        "```bash\n",
        "module list\n",
        "```\n",
        "Make sure that the hdf5-mpi module is listed.\n",
        "\n",
        "###Step 3: Create the C File with Nano\n",
        "Now, we will create the HDF5 C program. Use nano to create a new file named parallel_hdf5_example.c.\n",
        "\n",
        "```bash\n",
        "nano parallel_hdf5_example.c\n",
        "```\n",
        "Copy and paste the following sample HDF5 program into the file:\n",
        "\n",
        "```c\n",
        "#include <mpi.h>\n",
        "#include <hdf5.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    hid_t file_id; /* File identifier */\n",
        "    herr_t status;\n",
        "\n",
        "    /* Create a new file collectively using default properties. */\n",
        "    file_id = H5Fcreate(\"test_parallel.h5\", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);\n",
        "\n",
        "    /* Close the file */\n",
        "    status = H5Fclose(file_id);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "To save and exit in nano, press CTRL + O to save, then CTRL + X to exit.\n",
        "\n",
        "###Step 4: Compile the HDF5 Program\n",
        "Now that the file is created, compile it using mpicc (the MPI-enabled C compiler) with the following command:\n",
        "\n",
        "```bash\n",
        "mpicc parallel_hdf5_example.c -o parallel_hdf5_example -lhdf5 -lhdf5_hl\n",
        "```\n",
        "\n",
        "This command compiles the program and links the necessary HDF5 libraries.\n",
        "\n",
        "If you receive any errors, make sure the hdf5-mpi module is loaded, and the correct paths are set.\n",
        "\n",
        "###Step 5: Run the Program\n",
        "Once the program is compiled, run it using mpirun:\n",
        "\n",
        "```bash\n",
        "mpirun -np 2 ./parallel_hdf5_example\n",
        "```\n",
        "The -np 2 flag tells mpirun to run the program on 2 processes. You can adjust the number of processes as needed. If it fails you can try to run it with -oversubscribe option\n",
        "\n",
        "###Step 6: Verify the Output\n",
        "If everything works correctly, a file named test_parallel.h5 should be created in your current directory. To verify it, list the files in the directory:\n",
        "\n",
        "```bash\n",
        "ls\n",
        "```\n",
        "You should see test_parallel.h5 in the output. If it's there, the program ran successfully.\n",
        "\n",
        "###Step 7: Clean Up (Optional)\n",
        "To remove the generated files, you can run:\n",
        "\n",
        "```bash\n",
        "rm parallel_hdf5_example test_parallel.h5\n",
        "```\n",
        "This removes both the compiled binary and the generated HDF5 file."
      ],
      "metadata": {
        "id": "a5oYcQIRWdC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TuMahCdzd5uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guidelines for Designing I/O Systems in HPC\n",
        "\n",
        "## Introduction\n",
        "In this practice, we will explore the key guidelines for designing efficient I/O systems in high-performance computing (HPC) environments. Efficient I/O is crucial for improving performance in HPC systems, particularly when dealing with massive datasets that are common in scientific and engineering applications.\n",
        "\n",
        "We will focus on the following concepts:\n",
        "- **Optimizing Data Locality & Access Patterns**: Placing data close to compute resources to reduce latency.\n",
        "- **Leveraging Parallel I/O Techniques**: Using libraries like MPI-IO and NetCDF to perform parallel file operations.\n",
        "- **Implementing I/O Scheduling & Load Balancing**: Distributing I/O operations across resources to balance the load.\n",
        "- **Incorporating Resilience & Fault Tolerance**: Implementing mechanisms to handle I/O failures and ensure data integrity.\n",
        "\n",
        "The goal of this exercise is to learn about these principles through hands-on experience. We will write and compile C programs that demonstrate these concepts and execute them in a Google Colab environment.\n"
      ],
      "metadata": {
        "id": "VsgUUgu_d2yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary tools in Google Colab\n",
        "!apt-get install mpich\n",
        "!apt-get install gcc\n"
      ],
      "metadata": {
        "id": "jlwHTgeSeDyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the C file to demonstrate parallel I/O using MPI-IO with enhanced logging\n",
        "\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "#define FILENAME \"testfile.bin\"\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    int rank, size;\n",
        "    MPI_File file;\n",
        "    MPI_Status status;\n",
        "    MPI_Init(&argc, &argv);  // Initialize MPI\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);  // Get the rank of the process\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);  // Get the size of the communicator\n",
        "\n",
        "    // Inform the user about the number of processes\n",
        "    if (rank == 0) {\n",
        "        printf(\"Running Parallel I/O with %d processes...\\\\n\", size);\n",
        "    }\n",
        "\n",
        "    // Open the file for parallel I/O\n",
        "    MPI_File_open(MPI_COMM_WORLD, FILENAME, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);\n",
        "\n",
        "    // Each process writes its rank multiplied by 10 to a different part of the file\n",
        "    int buf = rank * 10;\n",
        "    MPI_Offset offset = rank * sizeof(int);\n",
        "\n",
        "    // Print logging information before writing\n",
        "    printf(\"Process %d writing value %d at offset %lld\\\\n\", rank, buf, (long long)offset);\n",
        "\n",
        "    // Perform the write operation\n",
        "    MPI_File_write_at(file, offset, &buf, 1, MPI_INT, &status);\n",
        "\n",
        "    // Print logging information after writing\n",
        "    printf(\"Process %d finished writing\\\\n\", rank);\n",
        "\n",
        "    // Close the file\n",
        "    MPI_File_close(&file);\n",
        "\n",
        "    MPI_Finalize();  // Finalize MPI\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write the C code to a file\n",
        "with open(\"parallel_io.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C code\n",
        "!mpicc -o parallel_io parallel_io.c\n",
        "\n",
        "# Run the parallel I/O program with 4 processes, allowing root execution and printing details\n",
        "!mpirun --allow-run-as-root -np 4 -oversubscribe ./parallel_io\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qo6VyiIfJJe",
        "outputId": "220de268-1207-4557-c2ba-a367837bb4a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Parallel I/O with 4 processes...\n",
            "Process 3 writing value 30 at offset 12\n",
            "Process 0 writing value 0 at offset 0\n",
            "Process 3 finished writing\n",
            "Process 1 writing value 10 at offset 4\n",
            "Process 1 finished writing\n",
            "Process 0 finished writing\n",
            "Process 2 writing value 20 at offset 8\n",
            "Process 2 finished writing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Code\n",
        "\n",
        "In this section, we will go through the key components of the C code used to demonstrate parallel I/O in an HPC environment:\n",
        "\n",
        "### MPI Initialization\n",
        "```c\n",
        "MPI_Init(&argc, &argv);\n",
        "```\n",
        "\n",
        "This initializes the MPI environment, which is required for any parallel operations using MPI. Every MPI program must call this at the beginning.\n",
        "\n",
        "Opening the File for Parallel I/O\n",
        "```c\n",
        "MPI_File_open(MPI_COMM_WORLD, FILENAME, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);\n",
        "```\n",
        "\n",
        "Here, we open a file (testfile.bin) in parallel using MPI_File_open. The flag MPI_MODE_CREATE | MPI_MODE_WRONLY indicates that the file should be created if it doesn't exist and opened in write-only mode.\n",
        "\n",
        "Writing Data by Each Process\n",
        "\n",
        "```c\n",
        "MPI_File_write_at(file, rank * sizeof(int), &buf, 1, MPI_INT, &status);\n",
        "```\n",
        "\n",
        "Each process writes its data at a different offset in the file. The offset is determined by rank * sizeof(int), so that each process writes its data to a unique location. This demonstrates parallel I/O, where multiple processes can write to the same file simultaneously without conflict.\n",
        "\n",
        "Finalizing MPI\n",
        "\n",
        "```c\n",
        "MPI_Finalize();\n",
        "```\n",
        "Once all the operations are complete, we call MPI_Finalize to clean up the MPI environment.\n",
        "\n",
        "Running the Program\n",
        "The program is executed using mpirun with 4 processes, as shown below:\n",
        "\n",
        "```bash\n",
        "mpirun -np 4 ./parallel_io\n",
        "```\n",
        "This means that 4 processes will be used to write their respective data to the file concurrently. After the program runs, each process will have written its rank multiplied by 10 to the file at its corresponding offset."
      ],
      "metadata": {
        "id": "RV6p-SaDeHyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-Phase Commit Protocol in HPC\n",
        "\n",
        "## Introduction\n",
        "The two-phase commit protocol is essential in distributed systems like High-Performance Computing (HPC), where ensuring atomicity and consistency of transactions across multiple nodes is critical. This protocol is used in scenarios such as checkpointing—saving the current state of a computation across many nodes—ensuring that the checkpoint operation is either committed or aborted simultaneously on all nodes.\n",
        "\n",
        "### Why is it important?\n",
        "In distributed HPC environments, if nodes become out of sync during operations like checkpointing, it can lead to incorrect or inconsistent computation results. The two-phase commit ensures that either all nodes commit a transaction (e.g., saving a checkpoint) or all nodes abort it if any node fails to do so, preserving data integrity across the system.\n",
        "\n",
        "In this exercise, we will simulate a simplified version of the two-phase commit protocol using Python to represent a distributed checkpoint operation.\n",
        "\n",
        "## How It Works\n",
        "The protocol works in two phases:\n",
        "1. **Preparation Phase**: Each node prepares to perform the checkpoint, responding with either \"YES\" (ready to commit) or \"NO\" (cannot commit).\n",
        "2. **Commit/Abort Phase**: If all nodes vote \"YES,\" the coordinator instructs them to commit the checkpoint. If any node votes \"NO,\" the coordinator instructs all nodes to abort the checkpoint.\n",
        "\n",
        "We will simulate this using a coordinator node and multiple worker nodes.\n"
      ],
      "metadata": {
        "id": "Zt7V5AsUdy8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating Two-Phase Commit Protocol in HPC for Checkpointing\n",
        "\n",
        "import random\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.checkpoint_ready = False\n",
        "\n",
        "    def prepare_checkpoint(self, data):\n",
        "        \"\"\" Simulate preparation for checkpointing \"\"\"\n",
        "        # Randomly decide if the node can save the checkpoint\n",
        "        self.checkpoint_ready = random.choice([True, False])\n",
        "        print(f\"{self.name}: Preparing checkpoint... {'YES' if self.checkpoint_ready else 'NO'}\")\n",
        "        return \"YES\" if self.checkpoint_ready else \"NO\"\n",
        "\n",
        "    def commit_checkpoint(self, data):\n",
        "        \"\"\" Commit checkpoint if ready \"\"\"\n",
        "        if self.checkpoint_ready:\n",
        "            print(f\"{self.name}: Committing checkpoint...\")\n",
        "        else:\n",
        "            print(f\"{self.name}: Cannot commit, not ready!\")\n",
        "\n",
        "    def abort_checkpoint(self):\n",
        "        \"\"\" Abort checkpoint operation \"\"\"\n",
        "        print(f\"{self.name}: Aborting checkpoint...\")\n",
        "\n",
        "class Coordinator:\n",
        "    def __init__(self, nodes):\n",
        "        self.nodes = nodes\n",
        "\n",
        "    def perform_checkpoint(self, checkpoint_data):\n",
        "        print(\"Coordinator: Initiating checkpoint...\")\n",
        "        votes = []\n",
        "        for node in self.nodes:\n",
        "            vote = node.prepare_checkpoint(checkpoint_data)\n",
        "            votes.append(vote)\n",
        "\n",
        "        # Check if all nodes voted \"YES\"\n",
        "        if all(vote == \"YES\" for vote in votes):\n",
        "            print(\"Coordinator: All nodes voted YES. Committing checkpoint...\")\n",
        "            for node in self.nodes:\n",
        "                node.commit_checkpoint(checkpoint_data)\n",
        "        else:\n",
        "            print(\"Coordinator: Some nodes voted NO. Aborting checkpoint...\")\n",
        "            for node in self.nodes:\n",
        "                node.abort_checkpoint()\n",
        "\n",
        "# Simulating the two-phase commit with nodes\n",
        "nodes = [Node(f\"Node {i+1}\") for i in range(4)]  # Create 4 nodes\n",
        "coordinator = Coordinator(nodes)\n",
        "\n",
        "# Perform checkpoint\n",
        "checkpoint_data = \"checkpoint_data\"\n",
        "coordinator.perform_checkpoint(checkpoint_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61H8T3PijBHp",
        "outputId": "8e4211d4-e3d2-4b0b-e17f-431e50c4a36b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coordinator: Initiating checkpoint...\n",
            "Node 1: Preparing checkpoint... YES\n",
            "Node 2: Preparing checkpoint... NO\n",
            "Node 3: Preparing checkpoint... NO\n",
            "Node 4: Preparing checkpoint... YES\n",
            "Coordinator: Some nodes voted NO. Aborting checkpoint...\n",
            "Node 1: Aborting checkpoint...\n",
            "Node 2: Aborting checkpoint...\n",
            "Node 3: Aborting checkpoint...\n",
            "Node 4: Aborting checkpoint...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Two-Phase Commit Code\n",
        "\n",
        "In this simulation, we modeled a simplified version of the two-phase commit protocol using a coordinator and multiple nodes. Each node decides whether it is ready to commit a checkpoint based on a random decision.\n",
        "\n",
        "### Key Components:\n",
        "1. **Node Class**:\n",
        "    - `prepare_checkpoint`: Each node prepares for the checkpoint and returns \"YES\" if ready, otherwise \"NO\".\n",
        "    - `commit_checkpoint`: If the node is ready, it commits the checkpoint.\n",
        "    - `abort_checkpoint`: If the node is not ready, it aborts the checkpoint operation.\n",
        "  \n",
        "2. **Coordinator Class**:\n",
        "    - `perform_checkpoint`: The coordinator manages the entire operation. It asks each node to prepare for the checkpoint and collects their votes. If all nodes vote \"YES,\" it instructs them to commit the checkpoint. If any node votes \"NO,\" it aborts the checkpoint on all nodes.\n",
        "\n",
        "### Output:\n",
        "When you run the program, you will see messages from the coordinator and nodes about whether they are ready to commit or abort the checkpoint. This simulates how a two-phase commit would work in an actual HPC system where the consistency of checkpoints across nodes is crucial.\n",
        "\n",
        "The two-phase commit protocol ensures that either all nodes commit or all abort, preventing inconsistent states across the cluster.\n"
      ],
      "metadata": {
        "id": "XI-6XU0gi-Vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consensus Algorithms in HPC\n",
        "\n",
        "## Introduction\n",
        "In distributed systems, such as High-Performance Computing (HPC) environments, it is often necessary for nodes to reach a consensus on shared states or configurations. Consensus algorithms like Paxos and Raft are used to ensure agreement across nodes, even in the presence of failures.\n",
        "\n",
        "### Why is Consensus Important in HPC?\n",
        "Consensus algorithms are vital in HPC for tasks like distributed job scheduling, maintaining consistency of shared state, or synchronizing tasks across many nodes. Without proper coordination, the system could face inconsistencies, leading to resource contention or incorrect computations.\n",
        "\n",
        "In this example, we will simulate a simplified version of a consensus algorithm using a class-based Python model to illustrate how nodes in an HPC system might reach consensus to elect a leader that coordinates tasks across a cluster.\n"
      ],
      "metadata": {
        "id": "WmOegNh-jQDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the Python code to a file\n",
        "consensus_code = \"\"\"\n",
        "import random\n",
        "import time\n",
        "\n",
        "class HPCNode:\n",
        "    def __init__(self, node_id):\n",
        "        self.node_id = node_id\n",
        "        self.state = \"follower\"\n",
        "        self.current_term = 0\n",
        "        self.voted_for = None\n",
        "\n",
        "    def start_election(self, cluster_nodes):\n",
        "        ''' Initiates leader election '''\n",
        "        self.state = \"candidate\"\n",
        "        self.current_term += 1\n",
        "        print(f\"Node {self.node_id} is starting an election (Term {self.current_term})...\")\n",
        "\n",
        "        votes = 1  # Vote for self\n",
        "\n",
        "        # Request votes from other nodes\n",
        "        for node in cluster_nodes:\n",
        "            if node != self and node.vote_request(self.current_term, self.node_id):\n",
        "                votes += 1\n",
        "\n",
        "        # Majority wins\n",
        "        if votes > len(cluster_nodes) // 2:\n",
        "            self.state = \"leader\"\n",
        "            print(f\"Node {self.node_id} is elected leader with {votes} votes!\")\n",
        "            self.coordinate_tasks(cluster_nodes)\n",
        "        else:\n",
        "            print(f\"Node {self.node_id} failed to become leader.\")\n",
        "\n",
        "    def vote_request(self, term, candidate_id):\n",
        "        ''' Respond to a vote request '''\n",
        "        if term > self.current_term:\n",
        "            self.current_term = term\n",
        "            self.voted_for = candidate_id\n",
        "            print(f\"Node {self.node_id} votes for Node {candidate_id}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def coordinate_tasks(self, cluster_nodes):\n",
        "        ''' Leader coordinates tasks across the cluster '''\n",
        "        print(f\"Node {self.node_id} is now coordinating tasks...\")\n",
        "        for i in range(3):  # Simulate coordinating 3 tasks\n",
        "            print(f\"Node {self.node_id}: Coordinating task {i+1}\")\n",
        "            time.sleep(1)  # Simulate time taken to coordinate tasks\n",
        "\n",
        "# Create nodes and simulate an election\n",
        "nodes = [HPCNode(i) for i in range(5)]  # Create 5 nodes in the cluster\n",
        "random_node = random.choice(nodes)  # Randomly select a node to start an election\n",
        "random_node.start_election(nodes)\n",
        "\"\"\"\n",
        "\n",
        "# Save the code to a Python file\n",
        "with open(\"consensus_algorithm.py\", \"w\") as f:\n",
        "    f.write(consensus_code)\n",
        "\n",
        "# Run the Python file\n",
        "!python3 consensus_algorithm.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf7UBbhOjH45",
        "outputId": "29842067-ae9a-41fa-c00c-40781a5ba3a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 2 is starting an election (Term 1)...\n",
            "Node 0 votes for Node 2\n",
            "Node 1 votes for Node 2\n",
            "Node 3 votes for Node 2\n",
            "Node 4 votes for Node 2\n",
            "Node 2 is elected leader with 5 votes!\n",
            "Node 2 is now coordinating tasks...\n",
            "Node 2: Coordinating task 1\n",
            "Node 2: Coordinating task 2\n",
            "Node 2: Coordinating task 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Consensus Algorithm Code\n",
        "\n",
        "In this simulation, we implemented a simple consensus algorithm for leader election in an HPC environment, similar to the Raft algorithm. Nodes elect a leader to coordinate tasks.\n",
        "\n",
        "### Key Components:\n",
        "1. **HPCNode Class**:\n",
        "    - `start_election`: The node becomes a candidate and requests votes from other nodes. If it receives a majority of votes, it becomes the leader.\n",
        "    - `vote_request`: Nodes respond to vote requests and grant their vote to the candidate with the highest term.\n",
        "    - `coordinate_tasks`: Once a node is elected leader, it coordinates tasks across the cluster.\n",
        "\n",
        "2. **Leader Election**:\n",
        "    - A node randomly initiates the election. If the node receives votes from a majority of nodes, it becomes the leader and begins coordinating tasks.\n",
        "    - If the node fails to receive a majority, the election fails, and the process can be restarted.\n",
        "\n",
        "### Output:\n",
        "When you run the program, you will see messages indicating the election process, including which nodes voted for the candidate and whether a leader was successfully elected. Once elected, the leader will begin coordinating tasks.\n",
        "\n",
        "### Importance in HPC:\n",
        "Consensus algorithms ensure that tasks are coordinated effectively across distributed nodes in an HPC environment. This is critical for job scheduling, resource management, and maintaining consistency across the system.\n"
      ],
      "metadata": {
        "id": "QiDseqX3jLLS"
      }
    }
  ]
}